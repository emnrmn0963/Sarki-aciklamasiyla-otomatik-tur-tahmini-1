{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "179f6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneler indirilerek veri ön işleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7893a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1f5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import csv\n",
    "# kütüphanelerimizi indirdik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "175a3370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') # nltk işlemlerini yapabilmek için ayarlamalar yaptık."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bfd650b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>appears on</th>\n",
       "      <th>artist</th>\n",
       "      <th>writers</th>\n",
       "      <th>producer</th>\n",
       "      <th>released</th>\n",
       "      <th>streak</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Like a Rolling Stone</td>\n",
       "      <td>\"I wrote it. I didn't fail. It was straight,\"�...</td>\n",
       "      <td>Highway 61 Revisited (Columbia)</td>\n",
       "      <td>Bob Dylan</td>\n",
       "      <td>Dylan</td>\n",
       "      <td>Tom Wilson</td>\n",
       "      <td>July, 1965</td>\n",
       "      <td>12 weeks</td>\n",
       "      <td>No. 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(I Can't Get No) Satisfaction'</td>\n",
       "      <td>\"It's the riff heard round the world,\" says St...</td>\n",
       "      <td>Out of Our Heads�(ABKCO)</td>\n",
       "      <td>The Rolling Stones</td>\n",
       "      <td>Mick Jagger, Keith Richards</td>\n",
       "      <td>Andrew Loog Oldham</td>\n",
       "      <td>May, 1965</td>\n",
       "      <td>14 weeks</td>\n",
       "      <td>No. 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Imagine</td>\n",
       "      <td>John Lennon wrote \"Imagine,\" his greatest musi...</td>\n",
       "      <td>Imagine�(Capitol/Apple)</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>Lennon, Phil Spector, Yoko Ono</td>\n",
       "      <td>October, 1971</td>\n",
       "      <td>9 weeks</td>\n",
       "      <td>No. 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What's Going On</td>\n",
       "      <td>\"What's Going On\" is an exquisite plea for pea...</td>\n",
       "      <td>What's Going On�(Tamla)</td>\n",
       "      <td>Marvin Gaye</td>\n",
       "      <td>Gaye, Renaldo Benson, Al Cleveland</td>\n",
       "      <td>Gaye�</td>\n",
       "      <td>Feb, 1971</td>\n",
       "      <td>13 weeks</td>\n",
       "      <td>No. 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Respect</td>\n",
       "      <td>Otis Redding wrote \"Respect\" and recorded it f...</td>\n",
       "      <td>I Never Loved a Man the Way I Love You�(Atlantic)</td>\n",
       "      <td>Aretha Franklin</td>\n",
       "      <td>Otis Redding</td>\n",
       "      <td>Jerry Wexler</td>\n",
       "      <td>April, 1967</td>\n",
       "      <td>12 weeks</td>\n",
       "      <td>No. 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title  \\\n",
       "0            Like a Rolling Stone   \n",
       "1  (I Can't Get No) Satisfaction'   \n",
       "2                         Imagine   \n",
       "3                 What's Going On   \n",
       "4                         Respect   \n",
       "\n",
       "                                         description  \\\n",
       "0  \"I wrote it. I didn't fail. It was straight,\"�...   \n",
       "1  \"It's the riff heard round the world,\" says St...   \n",
       "2  John Lennon wrote \"Imagine,\" his greatest musi...   \n",
       "3  \"What's Going On\" is an exquisite plea for pea...   \n",
       "4  Otis Redding wrote \"Respect\" and recorded it f...   \n",
       "\n",
       "                                          appears on              artist  \\\n",
       "0                    Highway 61 Revisited (Columbia)           Bob Dylan   \n",
       "1                           Out of Our Heads�(ABKCO)  The Rolling Stones   \n",
       "2                            Imagine�(Capitol/Apple)         John Lennon   \n",
       "3                            What's Going On�(Tamla)         Marvin Gaye   \n",
       "4  I Never Loved a Man the Way I Love You�(Atlantic)     Aretha Franklin   \n",
       "\n",
       "                              writers                        producer  \\\n",
       "0                               Dylan                     Tom Wilson    \n",
       "1         Mick Jagger, Keith Richards              Andrew Loog Oldham   \n",
       "2                         John Lennon  Lennon, Phil Spector, Yoko Ono   \n",
       "3  Gaye, Renaldo Benson, Al Cleveland                           Gaye�   \n",
       "4                        Otis Redding                    Jerry Wexler   \n",
       "\n",
       "        released    streak position  \n",
       "0     July, 1965  12 weeks    No. 2  \n",
       "1      May, 1965  14 weeks    No. 1  \n",
       "2  October, 1971   9 weeks    No. 3  \n",
       "3      Feb, 1971  13 weeks    No. 2  \n",
       "4    April, 1967  12 weeks    No. 1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('top500song.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "853485a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['description'].dropna().tolist()\n",
    "# şarkı açıklamaları üzerinde işlem yapmak istediğim için bu kolonu seçtim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "250aad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer ve Stemmer'ı başlatıyoruz\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51b83ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"I wrote it.',\n",
       " \"I didn't fail.\",\n",
       " 'It was straight,\"�Bob Dylan�said of his greatest song shortly after he recorded it in June 1965.',\n",
       " 'There is no better description of \"Like a Rolling Stone\" � of its revolutionary design and execution � or of the young man, just turned 24, who created it.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['description'][0]  # Açıklama kısmını ele alarak cümle ayırma işlemini gerçekleştiriyoruz.\n",
    "sentences = sent_tokenize(text)\n",
    "sentences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b558195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) # Stopwords listesini ingilizce olarak alıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10a63e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'having', 'hers', 'theirs', 'don', 'both', \"they'd\", \"he's\", 'mightn', 'any', 'most', 'if', 'each', \"weren't\", \"didn't\", 'yourself', 'who', 'ain', 'while', \"it'd\", \"we'd\", \"wouldn't\", 'to', 'below', \"i'd\", 'their', 'those', 'such', 'more', \"doesn't\", 'at', 'before', 'what', 'when', 'under', 'didn', \"they're\", 'my', 'but', 'out', 'isn', 'through', 'again', \"i'll\", 'all', \"hasn't\", 'herself', 'them', 'needn', 'you']\n"
     ]
    }
   ],
   "source": [
    "stop_words_list = list(stop_words)\n",
    "print(stop_words_list[:50]) # 50 tanesini görüyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "015bbe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences = [] # Kelimeleri tokenleştirip sadece harf olan kelimeleri alıyoruz ve stopword'leri çıkartıyoruz\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "     tokens = word_tokenize(sentence) #cümleleri kelimelere bölüyoruz ve boş bir liste oluşturuyoruz.\n",
    "     filtered_tokens = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8869075",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Kelimeleri tokenleştirip, lemmatize etme ve stemleme\n",
    "def preprocess_sentence(sentence):\n",
    "     tokens = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f0cff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created']]\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "        if token.isalpha():# Tokenlerin metin olup olmadığını kontrol ediyor (nümerik olan verileri işleme almıyor).\n",
    "            token_lower = token.lower() # küçük harfe çevirme işlemi\n",
    "            if token_lower not in stop_words: # Eğer küçük harfe çevrilmiş mevcut kelimeler stopword içinde yer almıyorsa\n",
    "                               filtered_tokens.append(token_lower) # filtered_tokens listesine yukarıda belirtilen \n",
    "                    #kurallara uygun yeni kelime ekle.\n",
    "        filtered_sentences.append(filtered_tokens) #filtre edilmiş cümleleri filtered_sentences listesine ekle.\n",
    "        \n",
    "print(filtered_sentences[:10]) # ilk 10 cümleyi ekrana yazdır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f56464fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created'], ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created']]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() # Her cümleyi lemetize et ve Lemmatizeri başlat\n",
    "tokenized_corpus_lemmatized = []  # Lemma edilmiş cümleleri saklamak için yeni bir boş liste oluştur.\n",
    "for filtered_tokens in filtered_sentences:\n",
    "    lemmatized_tokens = [] # Lemma edilmiş kelimeleri saklamak için boş bir liste oluştur.\n",
    "    for token in filtered_tokens:\n",
    "        lemma = lemmatizer.lemmatize(token) # Tokenleri tek tek lemma etme işlemi.\n",
    "        lemmatized_tokens.append(lemma) # Lemma edilmiş tokenleri lemmatized_tokens listesine ekleme işlemi.\n",
    "        tokenized_corpus_lemmatized.append(lemmatized_tokens) # Lemma edilmiş cümleleri \n",
    "        #tokenized_corpus_lemmatized ekleme işlemi.\n",
    "print(tokenized_corpus_lemmatized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a86b4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize edilmiş cümlelerin bir csv dosyasına kaydedilmesi.\n",
    "with open(r\"C:\\Users\\lenovo\\Desktop\\metin_verileri_ile_dogal_dil_isleme\\lemmatized_sentences.csv\", mode=\"w\",\n",
    "          newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Her cümleyi bir satır olarak yazma komutu.\n",
    "    for tokens in tokenized_corpus_lemmatized:\n",
    "        writer.writerow([' '.join(tokens)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5446ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat'], ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat'], ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat'], ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat'], ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat'], ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat'], ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat'], ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat'], ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat'], ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat']]\n"
     ]
    }
   ],
   "source": [
    "# Her cümleyi Stemle\n",
    "stemmer = PorterStemmer() # stemmeri başlat\n",
    "tokenized_corpus_stemmed = []\n",
    "\n",
    "for filtered_tokens in filtered_sentences:\n",
    "    stemmed_tokens = []\n",
    "    for token in filtered_tokens:\n",
    "        stem = stemmer.stem(token) #Tokenleri tek tek Stem etme ıslemi\n",
    "        stemmed_tokens.append(stem) \n",
    "    tokenized_corpus_stemmed.append(stemmed_tokens)\n",
    "print(tokenized_corpus_stemmed[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4eff0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem edilmiş cümleleri bir csv dosyasına kaydetme işlemi\n",
    "with open(r\"C:\\Users\\lenovo\\Desktop\\metin_verileri_ile_dogal_dil_isleme\\stemmed_sentences.csv\", mode=\"w\", newline=\"\", \n",
    "          encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Her cümleyi bir satır olarak yaz\n",
    "    for tokens in tokenized_corpus_stemmed:\n",
    "        writer.writerow([' '.join(tokens)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "599bdf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cümle 1 - Base: \"I wrote it.\n",
      "Cümle 1 - Lemmatized: ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created']\n",
      "Cümle 1 - Stemmed: ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat']\n",
      "\n",
      "\n",
      "Cümle 2 - Base: I didn't fail.\n",
      "Cümle 2 - Lemmatized: ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created']\n",
      "Cümle 2 - Stemmed: ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat']\n",
      "\n",
      "\n",
      "Cümle 3 - Base: It was straight,\"�Bob Dylan�said of his greatest song shortly after he recorded it in June 1965.\n",
      "Cümle 3 - Lemmatized: ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created']\n",
      "Cümle 3 - Stemmed: ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat']\n",
      "\n",
      "\n",
      "Cümle 4 - Base: There is no better description of \"Like a Rolling Stone\" � of its revolutionary design and execution � or of the young man, just turned 24, who created it.\n",
      "Cümle 4 - Lemmatized: ['better', 'description', 'like', 'rolling', 'stone', 'revolutionary', 'design', 'execution', 'young', 'man', 'turned', 'created']\n",
      "Cümle 4 - Stemmed: ['better', 'descript', 'like', 'roll', 'stone', 'revolutionari', 'design', 'execut', 'young', 'man', 'turn', 'creat']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f\"Cümle {i+1} - Base: {sentences[i]}\")  # Cümlelerin ilk halleri (ham veri)\n",
    "    \n",
    "    print(f\"Cümle {i+1} - Lemmatized: {tokenized_corpus_lemmatized[i]}\")  # Cümlelerin lemmatize halleri\n",
    "    \n",
    "    print(f\"Cümle {i+1} - Stemmed: {tokenized_corpus_stemmed[i]}\")  # Cümlelerin stemmed halleri\n",
    "    \n",
    "    print(\"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36704d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>better description like rolling stone revolutionary design execution young man turned created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>better description like rolling stone revoluti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>better description like rolling stone revoluti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>better description like rolling stone revoluti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>better description like rolling stone revoluti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>better description like rolling stone revoluti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  better description like rolling stone revolutionary design execution young man turned created\n",
       "0  better description like rolling stone revoluti...                                           \n",
       "1  better description like rolling stone revoluti...                                           \n",
       "2  better description like rolling stone revoluti...                                           \n",
       "3  better description like rolling stone revoluti...                                           \n",
       "4  better description like rolling stone revoluti...                                           "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmma edilmiş veriler üzerinde TF IDF vektörleme işlemi\n",
    "import pandas as pd\n",
    "dflemma = pd.read_csv(r\"C:\\Users\\lenovo\\Desktop\\metin_verileri_ile_dogal_dil_isleme\\lemmatized_sentences.csv\", encoding='utf-8')\n",
    "dflemma.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd39b899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['better description like rolling stone revolutionary design execution young man turned created',\n",
       " 'better description like rolling stone revolutionary design execution young man turned created',\n",
       " 'better description like rolling stone revolutionary design execution young man turned created']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Ön işlenmiş token listelerini tekrar metne çeviriyoruz\n",
    "lemmatized_texts = [' '.join(tokens) for tokens in tokenized_corpus_lemmatized]\n",
    "lemmatized_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f17bfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     better   created  description    design  execution      like       man  \\\n",
      "0  0.288675  0.288675     0.288675  0.288675   0.288675  0.288675  0.288675   \n",
      "1  0.288675  0.288675     0.288675  0.288675   0.288675  0.288675  0.288675   \n",
      "2  0.288675  0.288675     0.288675  0.288675   0.288675  0.288675  0.288675   \n",
      "3  0.288675  0.288675     0.288675  0.288675   0.288675  0.288675  0.288675   \n",
      "4  0.288675  0.288675     0.288675  0.288675   0.288675  0.288675  0.288675   \n",
      "\n",
      "   revolutionary   rolling     stone    turned     young  \n",
      "0       0.288675  0.288675  0.288675  0.288675  0.288675  \n",
      "1       0.288675  0.288675  0.288675  0.288675  0.288675  \n",
      "2       0.288675  0.288675  0.288675  0.288675  0.288675  \n",
      "3       0.288675  0.288675  0.288675  0.288675  0.288675  \n",
      "4       0.288675  0.288675  0.288675  0.288675  0.288675  \n"
     ]
    }
   ],
   "source": [
    "# TF-IDF vektörizer'ı başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "# TF-IDF matrisini oluşturuyoruz\n",
    "# Terim frekanslarını, belge frekanslarını hesaplar\n",
    "# TF-IDF vektörlerine dönüştürür\n",
    "tfidf_matrix = vectorizer.fit_transform(lemmatized_texts)\n",
    "## Kelimeleri alalım\n",
    "# TF-IDF vektörleştirme işleminde kullanılan tüm kelimelerin eşsiz bir listesini döndürür\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# TF-IDF matrisini pandas DataFrame'e çevir – görünürlük açısından – çalışması kolay\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "# İlk birkaç satırı gösterelim – ilk 5 cümle\n",
    "print(tfidf_df.head())\n",
    "tfidf_df.to_csv(\"tfidf_lemmatized.csv\",index=False)\n",
    "# Her satır bir cümleyi temsil eder\n",
    "# Her sütun bir kelimeyi temsil eder\n",
    "# Hücreler ise o kelimenin o cümledeki TF-IDF skorudur – her cümle için değişir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2faf3aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\n",
      "better         0.288675\n",
      "created        0.288675\n",
      "description    0.288675\n",
      "design         0.288675\n",
      "execution      0.288675\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# İlk cümle için TF-IDF skorlarını al\n",
    "first_sentence_vector = tfidf_df.iloc[0]\n",
    "# Skorlara göre sırala (yüksekten düşüğe)\n",
    "top_5_words = first_sentence_vector.sort_values(ascending=False).head(5)\n",
    "# Sonucu yazdır\n",
    "print(\"İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\")\n",
    "print(top_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39b2ae67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young: 1.0000\n",
      "turned: 1.0000\n",
      "stone: 1.0000\n",
      "rolling: 1.0000\n",
      "revolutionary: 1.0000\n",
      "man: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# like kelimesinin vektörünü alalım\n",
    "like_index = feature_names.tolist().index('like') # 'like' kelimesinin indeksini bul\n",
    "# like kelimesinin TF-IDF vektörünü alıyoruz ve 2D formatta yapıyoruz\n",
    "like_vector = tfidf_matrix[:, like_index].toarray()\n",
    "# Tüm kelimelerin TF-IDF vektörlerini alıyoruz\n",
    "tfidf_vectors = tfidf_matrix.toarray()\n",
    "# Cosine similarity hesaplayalım\n",
    "similarities = cosine_similarity(like_vector.T, tfidf_vectors.T)\n",
    "# Benzerlikleri sıralayalım ve en yüksek 5 kelimeyi seçelim\n",
    "similarities = similarities.flatten()\n",
    "top_5_indices = similarities.argsort()[-6:][::-1] # 6. en büyükten başlıyoruz çünkü kendisi de dahil\n",
    "# Sonuçları yazdıralım\n",
    "for index in top_5_indices:\n",
    "    print(f\"{feature_names[index]}: {similarities[index]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39e23703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemma edilmiş verileri Word2vec ile modelleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0115a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05e98bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dflemmavec = pd.read_csv(r\"C:\\Users\\lenovo\\Desktop\\metin_verileri_ile_dogal_dil_isleme\\lemmatized_sentences.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0c88d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec modeli eğitmek için parametreler\n",
    "parameters = [\n",
    "{'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "{'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "{'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "{'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "{'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "{'model_type': 'skipgram', 'window': 2, 'vector_size': 300},\n",
    "{'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "{'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25c49f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_model_cbow_window2_dim100.model saved!\n",
      "lemmatized_model_skipgram_window2_dim100.model saved!\n",
      "lemmatized_model_cbow_window4_dim100.model saved!\n",
      "lemmatized_model_skipgram_window4_dim100.model saved!\n",
      "lemmatized_model_cbow_window2_dim300.model saved!\n",
      "lemmatized_model_skipgram_window2_dim300.model saved!\n",
      "lemmatized_model_cbow_window4_dim300.model saved!\n",
      "lemmatized_model_skipgram_window4_dim300.model saved!\n",
      "stemmed_model_cbow_window2_dim100.model saved!\n",
      "stemmed_model_skipgram_window2_dim100.model saved!\n",
      "stemmed_model_cbow_window4_dim100.model saved!\n",
      "stemmed_model_skipgram_window4_dim100.model saved!\n",
      "stemmed_model_cbow_window2_dim300.model saved!\n",
      "stemmed_model_skipgram_window2_dim300.model saved!\n",
      "stemmed_model_cbow_window4_dim300.model saved!\n",
      "stemmed_model_skipgram_window4_dim300.model saved!\n"
     ]
    }
   ],
   "source": [
    "def train_and_save_model(corpus, params, model_name):\n",
    "    model = Word2Vec(corpus, vector_size=params['vector_size'],\n",
    " window=params['window'], min_count=1, sg=1 if params['model_type'] == 'skipgram' else 0)\n",
    "    \n",
    "    model.save(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}.model\")\n",
    "    print(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}.model saved!\")\n",
    "\n",
    "    \n",
    "#Lemmatize edilmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_lemmatized, param, \"lemmatized_model\")\n",
    "    # Stemlenmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_stemmed, param, \"stemmed_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cdb5adce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized CBOW Window 2 Dim 100 Modeli - 'like' ile En Benzer 3 Kelime:\n",
      "Kelime: revolutionary, Benzerlik Skoru: 0.9973840713500977\n",
      "Kelime: rolling, Benzerlik Skoru: 0.997376561164856\n",
      "Kelime: man, Benzerlik Skoru: 0.9972401857376099\n",
      "\n",
      "Stemmed Skipgram Window 4 Dim 100 Modeli - 'like' ile En Benzer 3 Kelime:\n",
      "Kelime: creat, Benzerlik Skoru: 0.22556331753730774\n",
      "Kelime: roll, Benzerlik Skoru: 0.05738338828086853\n",
      "Kelime: descript, Benzerlik Skoru: 0.025895075872540474\n",
      "\n",
      "Lemmatized Skipgram Window 2 Dim 300 Modeli - 'like' ile En Benzer 3 Kelime:\n",
      "Kelime: execution, Benzerlik Skoru: 0.9990801215171814\n",
      "Kelime: revolutionary, Benzerlik Skoru: 0.999017596244812\n",
      "Kelime: design, Benzerlik Skoru: 0.9990085959434509\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Model dosyalarını yüklemek\n",
    "model_1 = Word2Vec.load(\"lemmatized_model_cbow_window2_dim100.model\")\n",
    "model_2 = Word2Vec.load(\"stemmed_model_skipgram_window4_dim100.model\")\n",
    "model_3 = Word2Vec.load(\"lemmatized_model_skipgram_window2_dim300.model\")\n",
    "# 'like' kelimesi ile en benzer 3 kelimeyi ve skorlarını yazdırmak\n",
    "def print_similar_words(model, model_name):\n",
    "    similarity = model.wv.most_similar('like', topn=3)\n",
    "    print(f\"\\n{model_name} Modeli - 'like' ile En Benzer 3 Kelime:\")\n",
    "    for word, score in similarity:\n",
    "        print(f\"Kelime: {word}, Benzerlik Skoru: {score}\")\n",
    "# 3 model için benzer kelimeleri yazdır\n",
    "print_similar_words(model_1, \"Lemmatized CBOW Window 2 Dim 100\")\n",
    "print_similar_words(model_2, \"Stemmed Skipgram Window 4 Dim 100\")\n",
    "print_similar_words(model_3, \"Lemmatized Skipgram Window 2 Dim 300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff529665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem edilmiş verileri TF IDF yöntemiyle vektörleme #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "045fcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfstemm = pd.read_csv(r\"C:\\Users\\lenovo\\Desktop\\metin_verileri_ile_dogal_dil_isleme\\stemmed_sentences.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a7e4dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['better descript like roll stone revolutionari design execut young man turn creat',\n",
       " 'better descript like roll stone revolutionari design execut young man turn creat',\n",
       " 'better descript like roll stone revolutionari design execut young man turn creat']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Ön işlenmiş token listelerini tekrar metne çeviriyoruz\n",
    "stemmed_texts = [' '.join(tokens) for tokens in tokenized_corpus_stemmed]\n",
    "stemmed_texts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0cfcbc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     better     creat  descript    design    execut      like       man  \\\n",
      "0  0.288675  0.288675  0.288675  0.288675  0.288675  0.288675  0.288675   \n",
      "1  0.288675  0.288675  0.288675  0.288675  0.288675  0.288675  0.288675   \n",
      "2  0.288675  0.288675  0.288675  0.288675  0.288675  0.288675  0.288675   \n",
      "3  0.288675  0.288675  0.288675  0.288675  0.288675  0.288675  0.288675   \n",
      "4  0.288675  0.288675  0.288675  0.288675  0.288675  0.288675  0.288675   \n",
      "\n",
      "   revolutionari      roll     stone      turn     young  \n",
      "0       0.288675  0.288675  0.288675  0.288675  0.288675  \n",
      "1       0.288675  0.288675  0.288675  0.288675  0.288675  \n",
      "2       0.288675  0.288675  0.288675  0.288675  0.288675  \n",
      "3       0.288675  0.288675  0.288675  0.288675  0.288675  \n",
      "4       0.288675  0.288675  0.288675  0.288675  0.288675  \n"
     ]
    }
   ],
   "source": [
    "# TF-IDF vektörizerı başlatıyoruz\n",
    "vectorizer = TfidfVectorizer()\n",
    "# TF-IDF matrisini oluşturuyoruz\n",
    "#terim frekansları, belge frekanslarıni hesaplar\n",
    "#TF-IDF vektörlerine dönüştürür\n",
    "tfidf_matrix = vectorizer.fit_transform(stemmed_texts)\n",
    "## Kelimeleri alalım\n",
    "#F-IDF vektörleştirme işleminde kullanılan tüm kelimelerin essiz bir listesini döndürür\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# TF-IDF matrisini pandas DataFrame'e çevir-görünürlük açısından çalışılması kolay \n",
    "tfidf_df3 = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "# İlk birkaç satırı gösterelim-ilk 5 cümle\n",
    "print(tfidf_df3.head())\n",
    "tfidf_df.to_csv(\"tfidf_stemmed.csv\",index=False)\n",
    "#Her satır bir cümleyi temsil eder\n",
    "#Her sütun bir kelimeyi temsil eder\n",
    "#Hücreler ise o kelimenin o cümledeki TF-IDF skorudur - her cümle için değişir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d6c534c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\n",
      "better         0.288675\n",
      "created        0.288675\n",
      "description    0.288675\n",
      "design         0.288675\n",
      "execution      0.288675\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# İlk cümle için TF-IDF skorlarını al\n",
    "first_sentence_vector = tfidf_df.iloc[0]\n",
    "# Skorlara göre sırala (yüksekten düşüğe)\n",
    "top_5_words = first_sentence_vector.sort_values(ascending=False).head(5)\n",
    "# Sonucu yazdır\n",
    "print(\"İlk cümlede en yüksek TF-IDF skoruna sahip 5 kelime:\")\n",
    "print(top_5_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "650fa6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young: 1.0000\n",
      "turn: 1.0000\n",
      "stone: 1.0000\n",
      "roll: 1.0000\n",
      "revolutionari: 1.0000\n",
      "man: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# creat kelimesinin vektörünü alalım\n",
    "creat_index = feature_names.tolist().index('creat') # 'creat' kelimesinin indeksini bul\n",
    "# creat kelimesinin TF-IDF vektörünü alıyoruz ve 2D formatta yapıyoruz\n",
    "creat_vector = tfidf_matrix[:, creat_index].toarray()\n",
    "# Tüm kelimelerin TF-IDF vektörlerini alıyoruz\n",
    "tfidf_vectors = tfidf_matrix.toarray()\n",
    "# Cosine similarity hesaplayalım\n",
    "similarities = cosine_similarity(creat_vector.T, tfidf_vectors.T)\n",
    "# Benzerlikleri sıralayalım ve en yüksek 5 kelimeyi seçelim\n",
    "similarities = similarities.flatten()\n",
    "top_5_indices = similarities.argsort()[-6:][::-1] # 6. en büyükten başlıyoruz çünkü kendisi de dahil\n",
    "# Sonuçları yazdıralım\n",
    "for index in top_5_indices:\n",
    "    print(f\"{feature_names[index]}: {similarities[index]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "792b647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem edilmiş verileri Word2vec vektörü kullanarak modelleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "96a9a7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4d106b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfstemmsen = pd.read_csv(r\"C:\\Users\\lenovo\\Desktop\\metin_verileri_ile_dogal_dil_isleme\\stemmed_sentences.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30e02c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "{'model_type': 'cbow', 'window': 2, 'vector_size': 100},\n",
    "{'model_type': 'skipgram', 'window': 2, 'vector_size': 100},\n",
    "{'model_type': 'cbow', 'window': 4, 'vector_size': 100},\n",
    "{'model_type': 'skipgram', 'window': 4, 'vector_size': 100},\n",
    "{'model_type': 'cbow', 'window': 2, 'vector_size': 300},\n",
    "{'model_type': 'skipgram', 'window': 2, 'vector_size': 300},\n",
    "{'model_type': 'cbow', 'window': 4, 'vector_size': 300},\n",
    "{'model_type': 'skipgram', 'window': 4, 'vector_size': 300}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6869ebf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized_model_cbow_window2_dim100.model saved!\n",
      "lemmatized_model_skipgram_window2_dim100.model saved!\n",
      "lemmatized_model_cbow_window4_dim100.model saved!\n",
      "lemmatized_model_skipgram_window4_dim100.model saved!\n",
      "lemmatized_model_cbow_window2_dim300.model saved!\n",
      "lemmatized_model_skipgram_window2_dim300.model saved!\n",
      "lemmatized_model_cbow_window4_dim300.model saved!\n",
      "lemmatized_model_skipgram_window4_dim300.model saved!\n",
      "stemmed_model_cbow_window2_dim100.model saved!\n",
      "stemmed_model_skipgram_window2_dim100.model saved!\n",
      "stemmed_model_cbow_window4_dim100.model saved!\n",
      "stemmed_model_skipgram_window4_dim100.model saved!\n",
      "stemmed_model_cbow_window2_dim300.model saved!\n",
      "stemmed_model_skipgram_window2_dim300.model saved!\n",
      "stemmed_model_cbow_window4_dim300.model saved!\n",
      "stemmed_model_skipgram_window4_dim300.model saved!\n"
     ]
    }
   ],
   "source": [
    "def train_and_save_model(corpus, params, model_name):\n",
    "    model = Word2Vec(corpus, vector_size=params['vector_size'],\n",
    " window=params['window'], min_count=1, sg=1 if params['model_type'] == 'skipgram' else 0)\n",
    "    \n",
    "    model.save(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}.model\")\n",
    "    print(f\"{model_name}_{params['model_type']}_window{params['window']}_dim{params['vector_size']}.model saved!\")\n",
    "\n",
    "    \n",
    "#Lemmatize edilmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_lemmatized, param, \"lemmatized_model\")\n",
    "    # Stemlenmiş corpus ile modelleri eğitme ve kaydetme\n",
    "for param in parameters:\n",
    "    train_and_save_model(tokenized_corpus_stemmed, param, \"stemmed_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fb81a266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized CBOW Window 2 Dim 100 Modeli - 'young' ile En Benzer 3 Kelime:\n",
      "Kelime: rolling, Benzerlik Skoru: 0.9970795512199402\n",
      "Kelime: design, Benzerlik Skoru: 0.9969865679740906\n",
      "Kelime: turned, Benzerlik Skoru: 0.9969447255134583\n",
      "\n",
      "Stemmed Skipgram Window 4 Dim 100 Modeli - 'young' ile En Benzer 3 Kelime:\n",
      "Kelime: design, Benzerlik Skoru: 0.1529773324728012\n",
      "Kelime: turn, Benzerlik Skoru: 0.09720946848392487\n",
      "Kelime: roll, Benzerlik Skoru: 0.09652910381555557\n",
      "\n",
      "Lemmatized Skipgram Window 2 Dim 300 Modeli - 'young' ile En Benzer 3 Kelime:\n",
      "Kelime: revolutionary, Benzerlik Skoru: 0.9990411996841431\n",
      "Kelime: better, Benzerlik Skoru: 0.9989854097366333\n",
      "Kelime: rolling, Benzerlik Skoru: 0.9989804029464722\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Model dosyalarını yüklemek\n",
    "model_1 = Word2Vec.load(\"lemmatized_model_cbow_window2_dim100.model\")\n",
    "model_2 = Word2Vec.load(\"stemmed_model_skipgram_window4_dim100.model\")\n",
    "model_3 = Word2Vec.load(\"lemmatized_model_skipgram_window2_dim300.model\")\n",
    "# 'young' kelimesi ile en benzer 3 kelimeyi ve skorlarını yazdırmak\n",
    "def print_similar_words(model, model_name):\n",
    "    similarity = model.wv.most_similar('young', topn=3)\n",
    "    print(f\"\\n{model_name} Modeli - 'young' ile En Benzer 3 Kelime:\")\n",
    "    for word, score in similarity:\n",
    "        print(f\"Kelime: {word}, Benzerlik Skoru: {score}\")\n",
    "# 3 model için benzer kelimeleri yazdır\n",
    "print_similar_words(model_1, \"Lemmatized CBOW Window 2 Dim 100\")\n",
    "print_similar_words(model_2, \"Stemmed Skipgram Window 4 Dim 100\")\n",
    "print_similar_words(model_3, \"Lemmatized Skipgram Window 2 Dim 300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72768de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48ea2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a096aa70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
